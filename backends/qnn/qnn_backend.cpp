#include <fstream>
#include <sstream>
#include <filesystem>
#include <chrono>
#include <any>

#ifdef ENABLE_QNN
#include "PAL/DynamicLoading.hpp"
#include "DynamicLoadUtil.hpp"
#include "DataUtil.hpp"
#include "Utils.hpp"
#include "QnnTypeMacros.hpp"
#include <HTP/QnnHtpPerfInfrastructure.h>
#include <HTP/QnnHtpDevice.h>
#include <HTP/QnnHtpGraph.h>
#include <HTP/QnnHtpContext.h>
#include <QnnContext.h>
#include "qnn_backend.h"
#include "commondef.h"
#include "soc_detect.h"
#endif

#include "logger.h"
#include "half.hpp"

#ifdef _WIN32
#define USE_MMAP 0
#else
#define USE_MMAP 1
#include <sys/mman.h>
#include <fcntl.h>
#include <unistd.h>
#endif


#define DEFAULT_QNN_LOGLEVEL QNN_LOG_LEVEL_INFO

namespace rwkvmobile {

#ifdef ENABLE_QNN
using namespace qnn::tools;

static void logCallback(const char* fmt,
    QnnLog_Level_t level,
    uint64_t timestamp,
    va_list argp) {
    char buffer[1024];

    // TODO
    try {
        vsnprintf(buffer, sizeof(buffer), fmt, argp);
        LOGI("QNN Log: %s", buffer);
    } catch (const std::exception& e) {
    }

    return; // buggy
}

static void getTensorDims(std::vector<size_t>& dims,
    uint32_t* inDimensions,
    uint32_t rank) {
    if (nullptr == inDimensions) {
        LOGE("input dimensions is nullptr");
        return;
    }
    for (size_t r = 0; r < rank; r++) {
        dims.push_back(inDimensions[r]);
    }
}

static size_t getQnnDatatypeSize(Qnn_DataType_t dataType) {
    switch (dataType) {
        case QNN_DATATYPE_FLOAT_16:
        case QNN_DATATYPE_UFIXED_POINT_16:
        case QNN_DATATYPE_UINT_16:
        case QNN_DATATYPE_INT_16:
            return sizeof(uint16_t);
        case QNN_DATATYPE_FLOAT_32:
        case QNN_DATATYPE_INT_32:
        case QNN_DATATYPE_UINT_32:
            return sizeof(uint32_t);
        case QNN_DATATYPE_UFIXED_POINT_8:
        case QNN_DATATYPE_UINT_8:
        case QNN_DATATYPE_INT_8:
        case QNN_DATATYPE_BOOL_8:
            return sizeof(uint8_t);
        case QNN_DATATYPE_FLOAT_64:
        case QNN_DATATYPE_INT_64:
        case QNN_DATATYPE_UINT_64:
            return sizeof(uint64_t);
        default:
            LOGE("Unsupported data type");
            return 0;
    }
}

int qnn_backend::init(void * extra) {
    if (extra != nullptr) {
        qnnBackendPath = std::string((char *)extra);
        LOGI("Setting QNN Backend Path: %s\n", qnnBackendPath.c_str());
    } else {
        qnnBackendPath = "libQnnHtp.so";
        LOGI("Using default QNN Backend Path: %s\n", qnnBackendPath.c_str());
    }

    return RWKV_SUCCESS;
}

int qnn_backend::load_model(std::string model_path) {
    if (!std::filesystem::exists(model_path)) {
        return RWKV_ERROR_MODEL | RWKV_ERROR_IO;
    }

    if (qnnBackendPath.empty()) {
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INVALID_PARAMETERS;
    }

    bool is_context_cache = 
#ifdef WIN32
        model_path.find(".dll") == std::string::npos;
#else
        model_path.find(".so") == std::string::npos;
#endif

    // load QNN functions
    auto qnnStatusCode = dynamicloadutil::getQnnFunctionPointers(
        qnnBackendPath, model_path, &qnnFunctionPointers, &qnnBackendLibraryHandle, !is_context_cache, &qnnModelHandle);

    if (dynamicloadutil::StatusCode::SUCCESS != qnnStatusCode) {
        if (dynamicloadutil::StatusCode::FAIL_LOAD_BACKEND == qnnStatusCode) {
            LOGE("Error initializing QNN Function Pointers: could not load backend: %s", qnnBackendPath.c_str());
            return RWKV_ERROR_BACKEND | RWKV_ERROR_IO;
        } else if (dynamicloadutil::StatusCode::FAIL_LOAD_MODEL == qnnStatusCode) {
            LOGE("Error initializing QNN Function Pointers: could not load model:%s ", model_path.c_str());
            return RWKV_ERROR_MODEL | RWKV_ERROR_IO;
        } else {
            LOGE("Error initializing QNN Function Pointers");
            return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
        }
    }

    if (is_context_cache) {
        std::string qnnSystemLibPath = 
#ifdef WIN32
            qnnBackendPath.substr(0, qnnBackendPath.find("QnnHtp.dll")) + "QnnSystem.dll";
#else
            qnnBackendPath.substr(0, qnnBackendPath.find("libQnnHtp.so")) + "libQnnSystem.so";
#endif

        auto qnnSystemLibStatus = dynamicloadutil::getQnnSystemFunctionPointers(qnnSystemLibPath, &qnnFunctionPointers);
        if (dynamicloadutil::StatusCode::SUCCESS != qnnSystemLibStatus) {
            LOGE("Error initializing QNN System Function Pointers");
            return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
        }
    }

    bool usingHtp = qnnBackendPath.find("Htp") != std::string::npos;
    if (usingHtp) {
        LOGI("Using QNN HTP Backend");
    }
    else {
        LOGE("Do not use QNN CPU/GPU backends!");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }

    // initialize QNN logging
    auto logLevel = DEFAULT_QNN_LOGLEVEL;
    if (QNN_SUCCESS !=
        qnnFunctionPointers.qnnInterface.logCreate(logCallback, logLevel, &qnnLogHandle)) {
      LOGW("Unable to initialize logging in the backend.");
    }

    // initialize QNN backend
    auto qnnBackendStatus = qnnFunctionPointers.qnnInterface.backendCreate(
        qnnLogHandle, nullptr, &qnnBackendHandle);
    if (QNN_BACKEND_NO_ERROR != qnnBackendStatus) {
      LOGE("Could not initialize backend due to error = %lu", qnnBackendStatus);
      return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    LOGI("Initialize Backend Returned Status = %lu", qnnBackendStatus);

    if (nullptr != qnnFunctionPointers.qnnInterface.propertyHasCapability) {
        auto qnnDevicePropertyStatus = qnnFunctionPointers.qnnInterface.propertyHasCapability(QNN_PROPERTY_GROUP_DEVICE);
        if (QNN_PROPERTY_NOT_SUPPORTED == qnnDevicePropertyStatus) {
            LOGW("Device property is not supported");
        }
        if (QNN_PROPERTY_ERROR_UNKNOWN_KEY == qnnDevicePropertyStatus) {
            LOGE("Device property is not known to backend");
            return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
        }

        auto qnnCreateDeviceStatus = qnnFunctionPointers.qnnInterface.deviceCreate(qnnLogHandle, nullptr, &qnnDeviceHandle);
        if (QNN_SUCCESS != qnnCreateDeviceStatus && QNN_DEVICE_ERROR_UNSUPPORTED_FEATURE != qnnCreateDeviceStatus) {
            LOGE("Failed to create device");
            return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
        }
    }

    if (usingHtp) {
        if (RWKV_SUCCESS != qnn_create_power_config_id()) {
            LOGE("Could not create HTP power config id");
        } else {
            if (RWKV_SUCCESS != qnn_set_rpc_latency_and_polling()) {
                LOGE("Could not set HTP rpc latency and polling");
            } else if (RWKV_SUCCESS != qnn_set_power_config()) {
                LOGE("Could not set HTP power config");
            }
        }

#ifdef WIN32
        // TODO
#else
        soc_detect soc_detect;
        soc_detect.detect_platform();
        std::string htp_arch = soc_detect.get_htp_arch();
        std::string custom_op_name = "libQnnRwkvWkvOpPackage.so";
        if (htp_arch == "v75") {
            custom_op_name = "libQnnRwkvWkvOpPackageV75.so";
        } else if (htp_arch == "v73") {
            custom_op_name = "libQnnRwkvWkvOpPackageV73.so";
        } else if (htp_arch == "v69") {
            custom_op_name = "libQnnRwkvWkvOpPackageV69.so";
        } else if (htp_arch == "v68") {
            custom_op_name = "libQnnRwkvWkvOpPackageV68.so";
        }

        const char* ldLibraryPath = getenv("LD_LIBRARY_PATH");
        if (ldLibraryPath) {
            std::string pathStr(ldLibraryPath);
            std::stringstream ss(pathStr);
            std::string dir;
            while (std::getline(ss, dir, ':')) {
                std::string fullPath = dir + "/" + custom_op_name;
                std::ifstream file(fullPath);
                if (file.good()) {
                    LOGI("Found %s in LD_LIBRARY_PATH: %s", custom_op_name.c_str(), fullPath.c_str());
                    if (RWKV_SUCCESS != qnn_register_op_package(fullPath, "RwkvWkvOpPackageInterfaceProvider")) {
                        LOGE("Op package registration failed");
                    }
                    break;
                }
            }
        }
#endif
    }

    if (is_context_cache) {
        if (nullptr == qnnFunctionPointers.qnnSystemInterface.systemContextCreate ||
            nullptr == qnnFunctionPointers.qnnSystemInterface.systemContextGetBinaryInfo ||
            nullptr == qnnFunctionPointers.qnnSystemInterface.systemContextFree) {
            LOGE("QNN System function pointers are not populated.");
            return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
        }

        std::vector<std::shared_ptr<uint8_t>> buffer;
        std::vector<uint64_t> bufferSizes;

        int n_chunks = 1;
        auto pos = model_path.find("_chunk");
        if (pos != std::string::npos) {
            n_chunks = std::stoi(model_path.substr(model_path.find("of") + 2));
            LOGI("Number of chunks: %d", n_chunks);
        }

        buffer.resize(n_chunks);
        bufferSizes.resize(n_chunks);
        qnnContextHandles.resize(n_chunks);

        // read model binaries
        datautil::StatusCode binaryReadingStatus{datautil::StatusCode::SUCCESS};
        for (int i = 0; i < n_chunks; i++) {
            if (n_chunks > 1) {
                model_path = model_path.substr(0, pos) + "_chunk" + std::to_string(i+1) + "of" + std::to_string(n_chunks) + ".bin";
                std::cout << "Reading chunk: " << model_path << std::endl;
            }
            std::tie(binaryReadingStatus, bufferSizes[i]) = datautil::getFileSize(model_path);
            if (0 == bufferSizes[i]) {
                LOGE("Received path to an empty file. Nothing to deserialize.");
                return RWKV_ERROR_MODEL | RWKV_ERROR_IO;
            }
            std::cout << "Buffer size: " << bufferSizes[i] << std::endl;

#if USE_MMAP
            int fd = open(model_path.c_str(), O_RDONLY);
            if (fd < 0) {
                LOGE("Failed to open file %s", model_path.c_str());
                return RWKV_ERROR_MODEL | RWKV_ERROR_IO;
            }

            buffer[i] = std::shared_ptr<uint8_t>(
                (uint8_t*)mmap(NULL, bufferSizes[i], PROT_READ, MAP_SHARED, fd, 0), [bufferSizes, i](uint8_t* p) {
                    if (p) {
                        munmap(p, bufferSizes[i]);
                    }
                }
            );

            if (buffer[i].get() == MAP_FAILED) {
                LOGE("Failed to mmap file %s", model_path.c_str());
                close(fd);
                return RWKV_ERROR_MODEL | RWKV_ERROR_IO;
            }
#else
            buffer[i] = std::shared_ptr<uint8_t>(new uint8_t[bufferSizes[i]], std::default_delete<uint8_t[]>());
            if (!buffer[i]) {
                LOGE("Failed to allocate memory.");
                return RWKV_ERROR_MODEL | RWKV_ERROR_ALLOC;
            }

            binaryReadingStatus = datautil::readBinaryFromFile(
                model_path, reinterpret_cast<uint8_t*>(buffer[i].get()), bufferSizes[i]);
            if (binaryReadingStatus != datautil::StatusCode::SUCCESS) {
                LOGE("Failed to read binary data.");
                return RWKV_ERROR_MODEL | RWKV_ERROR_IO;
            }
#endif
        }

        // inspect binary info
        int returnStatus = RWKV_SUCCESS;
        std::vector<GraphInfo_t **> graphInfos(n_chunks);
        std::vector<uint32_t> graphCounts(n_chunks);

        for (int i = 0; i < n_chunks; i++)
        {
            QnnSystemContext_Handle_t sysCtxHandle{nullptr};
            if (QNN_SUCCESS != qnnFunctionPointers.qnnSystemInterface.systemContextCreate(&sysCtxHandle)) {
                LOGE("Could not create system handle.");
                returnStatus = RWKV_ERROR_MODEL | RWKV_ERROR_IO;
            }

            const QnnSystemContext_BinaryInfo_t* binaryInfo{nullptr};
            Qnn_ContextBinarySize_t binaryInfoSize{0};
            if (RWKV_SUCCESS == returnStatus &&
                QNN_SUCCESS != qnnFunctionPointers.qnnSystemInterface.systemContextGetBinaryInfo(
                                    sysCtxHandle,
                                    static_cast<void*>(buffer[i].get()),
                                    bufferSizes[i],
                                    &binaryInfo,
                                    &binaryInfoSize)) {
                LOGE("Failed to get context binary info");
                returnStatus = RWKV_ERROR_MODEL | RWKV_ERROR_IO;
            }

            // fill GraphInfo_t based on binary info
            if (RWKV_SUCCESS == returnStatus &&
                !qnn::tools::rwkv_app::copyMetadataToGraphsInfo(binaryInfo, graphInfos[i], graphCounts[i])) {
                LOGE("Failed to copy metadata.");
                returnStatus = RWKV_ERROR_MODEL;
            }
            qnnFunctionPointers.qnnSystemInterface.systemContextFree(sysCtxHandle);
            sysCtxHandle = nullptr;

            if (RWKV_SUCCESS == returnStatus &&
                nullptr == qnnFunctionPointers.qnnInterface.contextCreateFromBinary) {
                LOGE("contextCreateFromBinaryFnHandle is nullptr.");
                returnStatus = RWKV_ERROR_MODEL;
            }

            // QnnHtpContext_CustomConfig_t customConfig;
            // customConfig.option = QNN_HTP_CONTEXT_CONFIG_OPTION_IO_MEESTIMATION;
            // customConfig.ioMemEstimation = true;
            // QnnContext_Config_t* cfgs[] = {(QnnContext_Config_t*)&customConfig, NULL};

            if (RWKV_SUCCESS == returnStatus &&
                qnnFunctionPointers.qnnInterface.contextCreateFromBinary(
                    qnnBackendHandle,
                    qnnDeviceHandle,
                    nullptr, // (const QnnContext_Config_t**)cfgs,
                    static_cast<void*>(buffer[i].get()),
                    bufferSizes[i],
                    &qnnContextHandles[i],
                    nullptr)) {
                LOGE("Could not create context from binary.");
                returnStatus = RWKV_ERROR_MODEL;
            }

            isContextCreated = true;
            if (RWKV_SUCCESS == returnStatus) {
                for (size_t graphIdx = 0; graphIdx < graphCounts[i]; graphIdx++) {
                    if (nullptr == qnnFunctionPointers.qnnInterface.graphRetrieve) {
                        LOGE("graphRetrieveFnHandle is nullptr.");
                        returnStatus = RWKV_ERROR_MODEL;
                        break;
                    }
                    if (QNN_SUCCESS !=
                        qnnFunctionPointers.qnnInterface.graphRetrieve(
                            qnnContextHandles[i], (*graphInfos[i])[graphIdx].graphName, &((*graphInfos[i])[graphIdx].graph))) {
                        LOGE("Unable to retrieve graph handle for graph Idx: %zu", graphIdx);
                        returnStatus = RWKV_ERROR_MODEL;
                    }
                }
            }
            if (RWKV_SUCCESS != returnStatus) {
                LOGD("Cleaning up graph Info structures.");
                freeGraphsInfo(&graphInfos[i], graphCounts[i]);
            }
        }

        buffer.clear();

        qnnDecodeGraphsCount = 0;
        qnnPrefillGraphsCount = 0;
        for (int i = 0; i < n_chunks; i++) {
            for (int j = 0; j < graphCounts[i]; j++) {
                auto graphName = std::string((*graphInfos[i])[j].graphName);
                if (graphName.find("prefill") != std::string::npos) {
                    qnnPrefillGraphsCount++;
                } else {
                    qnnDecodeGraphsCount++;
                }
            }
        }

        qnnDecodeGraphsInfo = (GraphInfo_t **)calloc(qnnDecodeGraphsCount, sizeof(GraphInfo_t *));
        qnnPrefillGraphsInfo = (GraphInfo_t **)calloc(qnnPrefillGraphsCount, sizeof(GraphInfo_t *));
        GraphInfo_t *graphInfoArrDecode =
            (GraphInfo_t *)calloc(qnnDecodeGraphsCount, sizeof(GraphInfo_t));
        GraphInfo_t *graphInfoArrPrefill =
            (GraphInfo_t *)calloc(qnnPrefillGraphsCount, sizeof(GraphInfo_t));
        if (nullptr == qnnDecodeGraphsInfo || nullptr == qnnPrefillGraphsInfo || nullptr == graphInfoArrDecode || nullptr == graphInfoArrPrefill) {
            LOGE("Failure to allocate memory for *graphInfo");
            if (nullptr != qnnDecodeGraphsInfo) {
                free(qnnDecodeGraphsInfo);
            }
            if (nullptr != qnnPrefillGraphsInfo) {
                free(qnnPrefillGraphsInfo);
            }
            if (nullptr != graphInfoArrDecode) {
                free(graphInfoArrDecode);
            }
            if (nullptr != graphInfoArrPrefill) {
                free(graphInfoArrPrefill);
            }
            returnStatus = RWKV_ERROR_MODEL;
        }
        LOGI("qnnDecodeGraphsCount: %d, qnnPrefillGraphsCount: %d", qnnDecodeGraphsCount, qnnPrefillGraphsCount);

        if (RWKV_SUCCESS == returnStatus) {
            int prefill_gidx = 0, decode_gidx = 0;
            for (int i = 0; i < n_chunks; i++) {
                for (int j = 0; j < graphCounts[i]; j++) {
                    auto graphName = std::string((*graphInfos[i])[j].graphName);
                    if (graphName.find("prefill") != std::string::npos) {
                        qnnPrefillGraphsInfo[prefill_gidx] = graphInfoArrPrefill + prefill_gidx;
                        qnnPrefillGraphsInfo[prefill_gidx]->graph = (*graphInfos[i])[j].graph;
                        qnnPrefillGraphsInfo[prefill_gidx]->graphName = strdup((*graphInfos[i])[j].graphName);
                        qnnPrefillGraphsInfo[prefill_gidx]->inputTensors = (*graphInfos[i])[j].inputTensors;
                        qnnPrefillGraphsInfo[prefill_gidx]->numInputTensors = (*graphInfos[i])[j].numInputTensors;
                        qnnPrefillGraphsInfo[prefill_gidx]->outputTensors = (*graphInfos[i])[j].outputTensors;
                        qnnPrefillGraphsInfo[prefill_gidx]->numOutputTensors = (*graphInfos[i])[j].numOutputTensors;
                        prefill_gidx++;
                    } else {
                        qnnDecodeGraphsInfo[decode_gidx] = graphInfoArrDecode + decode_gidx;
                        qnnDecodeGraphsInfo[decode_gidx]->graph = (*graphInfos[i])[j].graph;
                        qnnDecodeGraphsInfo[decode_gidx]->graphName = strdup((*graphInfos[i])[j].graphName);
                        qnnDecodeGraphsInfo[decode_gidx]->inputTensors = (*graphInfos[i])[j].inputTensors;
                        qnnDecodeGraphsInfo[decode_gidx]->numInputTensors = (*graphInfos[i])[j].numInputTensors;
                        qnnDecodeGraphsInfo[decode_gidx]->outputTensors = (*graphInfos[i])[j].outputTensors;
                        qnnDecodeGraphsInfo[decode_gidx]->numOutputTensors = (*graphInfos[i])[j].numOutputTensors;
                        decode_gidx++;
                    }
                }
            }
        }
    } else {
        // create context
        qnnContextHandles.resize(1);
        if (QNN_CONTEXT_NO_ERROR != qnnFunctionPointers.qnnInterface.contextCreate(
                    qnnBackendHandle,
                    qnnDeviceHandle,
                    nullptr, // const QnnContext_Config_t**
                    &qnnContextHandles[0])) {
            LOGE("Could not create context");
            return RWKV_ERROR_BACKEND;
        }
        isContextCreated = true;

        // conpose graphs
        if (graphConfigsInfo == nullptr) {
            graphConfigsInfoCount = 2;

            graphConfigsInfo = new GraphConfigInfo_t*[graphConfigsInfoCount];
            graphConfigsInfo[0] = new GraphConfigInfo_t();
            graphConfigsInfo[0]->graphName = (char*)"model";
            graphConfigsInfo[0]->graphConfigs = (const QnnGraph_Config_t**)new QnnGraph_Config_t*[2];
            graphConfigsInfo[1] = new GraphConfigInfo_t();
            graphConfigsInfo[1]->graphName = (char*)"model_fp16";
            graphConfigsInfo[1]->graphConfigs = (const QnnGraph_Config_t**)new QnnGraph_Config_t*[2];
        
            static QnnHtpGraph_CustomConfig_t customConfig;
            customConfig.option = QNN_HTP_GRAPH_CONFIG_OPTION_PRECISION;
            customConfig.precision = QNN_PRECISION_FLOAT16;
            static QnnGraph_Config_t graphConfig;
            graphConfig.option = QNN_GRAPH_CONFIG_OPTION_CUSTOM;
            graphConfig.customConfig = &customConfig;
            for (int i = 0; i < graphConfigsInfoCount; i++) {
                graphConfigsInfo[i]->graphConfigs[0] = &graphConfig;
                graphConfigsInfo[i]->graphConfigs[1] = nullptr;
            }
        }

        if (ModelError_t::MODEL_NO_ERROR !=
            qnnFunctionPointers.composeGraphsFnHandle(
                qnnBackendHandle,
                qnnFunctionPointers.qnnInterface,
                qnnContextHandles[0],
                (const GraphConfigInfo_t**)graphConfigsInfo,
                graphConfigsInfoCount,
                &qnnDecodeGraphsInfo,
                &qnnDecodeGraphsCount,
                false,
                logCallback,
                DEFAULT_QNN_LOGLEVEL)) {
          LOGE("Failed in composeGraphs()");
          return RWKV_ERROR_MODEL;
        }

        // finalize graphs
        for (size_t graphIdx = 0; graphIdx < qnnDecodeGraphsCount; graphIdx++) {
            if (QNN_GRAPH_NO_ERROR !=
                qnnFunctionPointers.qnnInterface.graphFinalize(
                    (*qnnDecodeGraphsInfo)[graphIdx].graph, nullptr, nullptr)) {
                return RWKV_ERROR_MODEL;
            }
        }

        // save context cache
// #if WIN32
//         qnn_save_context_cache(model_path.substr(0, model_path.find('.dll')) + "_cache.bin");
// #else
//         qnn_save_context_cache(model_path.substr(0, model_path.find('.so')) + "_cache.bin");
// #endif

    }

    if (RWKV_SUCCESS != qnn_initialize_tensors()) {
        LOGE("Could not initialize tensors");
        return RWKV_ERROR_MODEL;
    }

    int state_tensor_count = 0;
    for (int i = 0; i < decodeGraphsTensorNameToTensorPointer.size(); i++) {
        for (auto &[tensorName, tensor] : decodeGraphsTensorNameToTensorPointer[i]) {
            if (tensorName.find("state") != std::string::npos && tensorName.find("in") != std::string::npos) {
                state_tensor_count++;
            }
        }
    }
    n_layers = state_tensor_count / 3;

    std::vector<size_t> dims_state;
    getTensorDims(dims_state, QNN_TENSOR_GET_DIMENSIONS((Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[0]["state1_in"]), QNN_TENSOR_GET_RANK((Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[0]["state1_in"]));
    num_heads = dims_state[0];
    hidden_size = num_heads * dims_state[1];

    std::vector<size_t> dims;
    getTensorDims(dims, QNN_TENSOR_GET_DIMENSIONS(logitsOutputTensor), QNN_TENSOR_GET_RANK(logitsOutputTensor));
    vocab_size = dims[2];
    return RWKV_SUCCESS;
}

void qnn_backend::fill_quantized_tensor(float value, Qnn_Tensor_t *tensor) {
    std::vector<size_t> dims;
    for (int j = 0; j < QNN_TENSOR_GET_RANK(*tensor); j++) {
        dims.push_back(*(QNN_TENSOR_GET_DIMENSIONS(*tensor) + j));
    }
    void *buffer = qnnIOTensorUtils.getBuffer(tensor);
    float fpzero = 0.0;
    auto dtype = QNN_TENSOR_GET_DATA_TYPE(*tensor);
    if (dtype == QNN_DATATYPE_UFIXED_POINT_8) {
        uint8_t qtzero = 0;
        datautil::floatToTfN<uint8_t>(&qtzero, &fpzero,
            QNN_TENSOR_GET_QUANT_PARAMS(*tensor).scaleOffsetEncoding.offset,
            QNN_TENSOR_GET_QUANT_PARAMS(*tensor).scaleOffsetEncoding.scale,
            1);
        for (int j = 0; j < datautil::calculateElementCount(dims); j++) {
            ((uint8_t*)buffer)[j] = qtzero;
        }
    } else if (dtype == QNN_DATATYPE_UFIXED_POINT_16) {
        uint16_t qtzero = 0;
        datautil::floatToTfN<uint16_t>(&qtzero, &fpzero,
            QNN_TENSOR_GET_QUANT_PARAMS(*tensor).scaleOffsetEncoding.offset,
            QNN_TENSOR_GET_QUANT_PARAMS(*tensor).scaleOffsetEncoding.scale,
            1);
        for (int j = 0; j < datautil::calculateElementCount(dims); j++) {
            ((uint16_t*)buffer)[j] = qtzero;
        }
    }
}

int qnn_backend::qnn_initialize_tensors() {
    if (!isTensorInitialized) {
        qnnIOTensorUtils.initialize(qnnContextHandles[0]);
        decodeGraphsTensorNameToTensorPointer.resize(qnnDecodeGraphsCount);
        decodeGraphsTensorNameToSize.resize(qnnDecodeGraphsCount);
        if (qnnPrefillGraphsCount > 0) {
            prefillGraphsTensorNameToTensorPointer.resize(qnnPrefillGraphsCount);
            prefillGraphsTensorNameToSize.resize(qnnPrefillGraphsCount);
        }

        for (int graph_id = 0; graph_id < qnnDecodeGraphsCount; graph_id++) {
            auto graphInfo     = (*qnnDecodeGraphsInfo)[graph_id];
            LOGD("Graph %d : %s", graph_id, graphInfo.graphName);

            for (size_t i = 0; i < graphInfo.numOutputTensors; i++) {
                size_t tensorDataSize = 1;
                for (int j = 0; j < QNN_TENSOR_GET_RANK(graphInfo.outputTensors[i]); j++) {
                    tensorDataSize *= *(QNN_TENSOR_GET_DIMENSIONS(graphInfo.outputTensors[i]) + j);
                }
                auto tensorName = std::string(QNN_TENSOR_GET_NAME(graphInfo.outputTensors[i]));
                size_t typeSize = getQnnDatatypeSize(QNN_TENSOR_GET_DATA_TYPE(graphInfo.outputTensors[i]));
                if (typeSize == 0) {
                    return RWKV_ERROR_IO;
                }
                tensorDataSize *= typeSize;
                decodeGraphsTensorNameToSize[graph_id][tensorName] = tensorDataSize;
                LOGI("Output Tensor %zu : %s Type: %d Size: %zu", i, tensorName.c_str(), QNN_TENSOR_GET_DATA_TYPE(graphInfo.outputTensors[i]), tensorDataSize);
            }

            if (!qnnIOTensorUtils.setupOutputTensors(&outputTensors[graph_id], decodeGraphsTensorNameToTensorPointer[graph_id], graphInfo,
                                          decodeGraphsTensorNameToSize[graph_id], qnnContextHandles[graph_id], false)) {
                LOGE("Error in setting up Output Tensors");
                return RWKV_ERROR_IO;
            }

            for (size_t i = 0; i < graphInfo.numOutputTensors; i++) {
                auto tensorName = std::string(QNN_TENSOR_GET_NAME(graphInfo.outputTensors[i]));
                if (tensorName == "out") {
                    logitsOutputTensor = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id]["out"];
                } else if (graph_id == qnnDecodeGraphsCount - 1 && tensorName.find("out_chunk") != std::string::npos) {
                    logitsOutputTensor = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id]["out_chunk" + std::to_string(graph_id+1)];
                }
            }

            std::unordered_map<std::string, Qnn_Tensor_t*> sharedTensorMap;
            for (size_t i = 0; i < graphInfo.numInputTensors; i++) {
                size_t tensorDataSize = 1;
                for (int j = 0; j < QNN_TENSOR_GET_RANK(graphInfo.inputTensors[i]); j++) {
                    tensorDataSize *= *(QNN_TENSOR_GET_DIMENSIONS(graphInfo.inputTensors[i]) + j);
                }
                auto tensorName = std::string(QNN_TENSOR_GET_NAME(graphInfo.inputTensors[i]));
                size_t typeSize = getQnnDatatypeSize(QNN_TENSOR_GET_DATA_TYPE(graphInfo.inputTensors[i]));
                if (typeSize == 0) {
                    return RWKV_ERROR_IO;
                }
                tensorDataSize *= typeSize;
                decodeGraphsTensorNameToSize[graph_id][tensorName] = tensorDataSize;
                LOGI("Input Tensor %zu : %s Type: %d Size: %zu", i, tensorName.c_str(), QNN_TENSOR_GET_DATA_TYPE(graphInfo.inputTensors[i]), tensorDataSize);
                if (tensorName.find("state") != std::string::npos) {
                    sharedTensorMap[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id][tensorName.substr(0, tensorName.find("_in")) + "_out"];
                }
                if (graph_id > 0) {
                    if (tensorName.find("v_first_in") != std::string::npos) {
                        sharedTensorMap[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[0]["v_first_out_chunk1"];
                    } else if (tensorName == "in_chunk" + std::to_string(graph_id + 1)) {
                        sharedTensorMap[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id - 1]["out_chunk" + std::to_string(graph_id)];
                    }
                }
            }

            if (!qnnIOTensorUtils.setupInputWithSharedTensors(&inputTensors[graph_id], decodeGraphsTensorNameToTensorPointer[graph_id], graphInfo,
                                        decodeGraphsTensorNameToSize[graph_id], qnnContextHandles[graph_id], sharedTensorMap)) {
                LOGE("Error in setting up Input Tensors");
                return RWKV_ERROR_IO;
            }

            for (size_t i = 0; i < graphInfo.numOutputTensors; i++) {
                // fill state tensors with zeros
                std::vector<size_t> dims;
                for (int j = 0; j < QNN_TENSOR_GET_RANK(outputTensors[graph_id][i]); j++) {
                    dims.push_back(*(QNN_TENSOR_GET_DIMENSIONS(outputTensors[graph_id][i]) + j));
                }
                void *buffer = qnnIOTensorUtils.getBuffer(&outputTensors[graph_id][i]);
                auto tensorName = std::string(QNN_TENSOR_GET_NAME(outputTensors[graph_id][i]));
                int state_num = 0;
                if (tensorName.find("state") != std::string::npos) {
                    tensorName = std::stoi(tensorName.substr(0, tensorName.find("_")).substr(5));
                }

                if (state_num % 3 == 1 || QNN_TENSOR_GET_DATA_TYPE(outputTensors[graph_id][i]) == QNN_DATATYPE_FLOAT_16)
                    memset(buffer, 0, datautil::calculateElementCount(dims) * sizeof(uint16_t));
                else if (QNN_TENSOR_GET_DATA_TYPE(outputTensors[graph_id][i]) == QNN_DATATYPE_FLOAT_32)
                    memset(buffer, 0, datautil::calculateElementCount(dims) * sizeof(float));
                else {
                    fill_quantized_tensor(0.0, &outputTensors[graph_id][i]);
                }
            }
        }

        if (qnnPrefillGraphsCount > 0) {
            std::unordered_map<std::string, Qnn_Tensor_t*> sharedTensorMapPrefill;
            for (int graph_id = 0; graph_id < qnnPrefillGraphsCount; graph_id++) {
                auto graphInfo     = (*qnnPrefillGraphsInfo)[graph_id];
                LOGI("Graph %d : %s", graph_id, graphInfo.graphName);
                for (size_t i = 0; i < graphInfo.numOutputTensors; i++) {
                    size_t tensorDataSize = 1;
                    for (int j = 0; j < QNN_TENSOR_GET_RANK(graphInfo.outputTensors[i]); j++) {
                        tensorDataSize *= *(QNN_TENSOR_GET_DIMENSIONS(graphInfo.outputTensors[i]) + j);
                    }
                    auto tensorName = std::string(QNN_TENSOR_GET_NAME(graphInfo.outputTensors[i]));
                    size_t typeSize = getQnnDatatypeSize(QNN_TENSOR_GET_DATA_TYPE(graphInfo.outputTensors[i]));
                    if (typeSize == 0) {
                        return RWKV_ERROR_IO;
                    }
                    tensorDataSize *= typeSize;
                    prefillGraphsTensorNameToSize[graph_id][tensorName] = tensorDataSize;
                    LOGI("Output Tensor %zu : %s Type: %d Size: %zu", i, tensorName.c_str(), QNN_TENSOR_GET_DATA_TYPE(graphInfo.outputTensors[i]), tensorDataSize);

                    if (tensorName.find("state") != std::string::npos) {
                        sharedTensorMapPrefill[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id][tensorName];
                    } else if (tensorName == "out_prefill") {
                        sharedTensorMapPrefill[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id]["out"];
                    } else if (graph_id == qnnPrefillGraphsCount - 1 && tensorName.find("out_prefill_chunk") != std::string::npos) {
                        sharedTensorMapPrefill[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id]["out_chunk" + std::to_string(graph_id+1)];
                    }
                }

                for (size_t i = 0; i < graphInfo.numInputTensors; i++) {
                    size_t tensorDataSize = 1;
                    for (int j = 0; j < QNN_TENSOR_GET_RANK(graphInfo.inputTensors[i]); j++) {
                        tensorDataSize *= *(QNN_TENSOR_GET_DIMENSIONS(graphInfo.inputTensors[i]) + j);
                    }
                    auto tensorName = std::string(QNN_TENSOR_GET_NAME(graphInfo.inputTensors[i]));
                    size_t typeSize = getQnnDatatypeSize(QNN_TENSOR_GET_DATA_TYPE(graphInfo.inputTensors[i]));
                    if (typeSize == 0) {
                        return RWKV_ERROR_IO;
                    }
                    tensorDataSize *= typeSize;
                    prefillGraphsTensorNameToSize[graph_id][tensorName] = tensorDataSize;
                    LOGI("Input Tensor %zu : %s Type: %d Size: %zu", i, tensorName.c_str(), QNN_TENSOR_GET_DATA_TYPE(graphInfo.inputTensors[i]), tensorDataSize);

                    if (tensorName.find("state") != std::string::npos) {
                        sharedTensorMapPrefill[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id][tensorName];
                    }

                    if (graph_id > 0) {
                        if (tensorName.find("v_first_in") != std::string::npos) {
                            sharedTensorMapPrefill[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[0]["v_first_out_prefill_chunk1"];
                        } else if (tensorName == "in_prefill_chunk" + std::to_string(graph_id + 1)) {
                            sharedTensorMapPrefill[tensorName] = (Qnn_Tensor_t*)decodeGraphsTensorNameToTensorPointer[graph_id - 1]["out_prefill_chunk" + std::to_string(graph_id)];
                        }
                    }
                }

                if (!qnnIOTensorUtils.setupOutputWithSharedTensors(&outputTensorsPrefill[graph_id], prefillGraphsTensorNameToTensorPointer[graph_id], graphInfo,
                                                prefillGraphsTensorNameToSize[graph_id], qnnContextHandles[graph_id], sharedTensorMapPrefill)) {
                    LOGE("Error in setting up Output Tensors");
                    return RWKV_ERROR_IO;
                }

                if (!qnnIOTensorUtils.setupInputWithSharedTensors(&inputTensorsPrefill[graph_id], prefillGraphsTensorNameToTensorPointer[graph_id], graphInfo,
                                                prefillGraphsTensorNameToSize[graph_id], qnnContextHandles[graph_id], sharedTensorMapPrefill)) {
                    LOGE("Error in setting up Input Tensors");
                    return RWKV_ERROR_IO;
                }

            }

            Qnn_Tensor_t *tensor = nullptr;
            if (prefillGraphsTensorNameToTensorPointer[0].find("in_prefill") != prefillGraphsTensorNameToTensorPointer[0].end()) {
                tensor = (Qnn_Tensor_t*)prefillGraphsTensorNameToTensorPointer[0]["in_prefill"];
            } else if (prefillGraphsTensorNameToTensorPointer[0].find("in_prefill_chunk1") != prefillGraphsTensorNameToTensorPointer[0].end()) {
                tensor = (Qnn_Tensor_t*)prefillGraphsTensorNameToTensorPointer[0]["in_prefill_chunk1"];
            }
            prefillSequenceLength = 1;
            for (int i = 0; i < QNN_TENSOR_GET_RANK(tensor); i++) {
                prefillSequenceLength *= *(QNN_TENSOR_GET_DIMENSIONS(tensor) + i);
            }
            LOGI("Prefill sequence length: %d", prefillSequenceLength);
        }
        isTensorInitialized = true;
    }

    return RWKV_SUCCESS;
}

int qnn_backend::eval(int id, std::vector<float> &logits) {
    if (!isTensorInitialized) {
        LOGD("qnn_backend::eval() isTensorInitialized: %d", isTensorInitialized);
        return RWKV_ERROR_EVAL;
    }
    int *token_input = (int*)qnnIOTensorUtils.getBuffer(&inputTensors[0][0]);
    *token_input = id;

    for (int graph_id = 0; graph_id < qnnDecodeGraphsCount; graph_id++) {
        auto graphInfo     = (*qnnDecodeGraphsInfo)[graph_id];
        auto executeStatus =
            qnnFunctionPointers.qnnInterface.graphExecute(graphInfo.graph,
                                                            inputTensors[graph_id],
                                                            graphInfo.numInputTensors,
                                                            outputTensors[graph_id],
                                                            graphInfo.numOutputTensors,
                                                            nullptr,
                                                            nullptr);

        if (QNN_GRAPH_NO_ERROR != executeStatus) {
            return RWKV_ERROR_IO;
        }
    }

    // copy logits
    if (logits.empty()) logits = std::vector<float>(vocab_size);

    if (QNN_TENSOR_GET_DATA_TYPE(logitsOutputTensor) == QNN_DATATYPE_FLOAT_32) {
        float *ptr = (float*)QNN_TENSOR_GET_CLIENT_BUF(logitsOutputTensor).data;
        logits = std::vector<float>(ptr, ptr + vocab_size);
    } else if (QNN_TENSOR_GET_DATA_TYPE(logitsOutputTensor) == QNN_DATATYPE_FLOAT_16) {
        half_float::half *ptr = (half_float::half*)QNN_TENSOR_GET_CLIENT_BUF(logitsOutputTensor).data;
        for (int i = 0; i < vocab_size; i++) {
            logits[i] = ptr[i];
        }
    } else if (QNN_TENSOR_GET_DATA_TYPE(logitsOutputTensor) == QNN_DATATYPE_UFIXED_POINT_16) {
        datautil::tfNToFloat<uint16_t>(logits.data(), reinterpret_cast<uint16_t*>(QNN_TENSOR_GET_CLIENT_BUF(logitsOutputTensor).data),
            QNN_TENSOR_GET_QUANT_PARAMS(logitsOutputTensor).scaleOffsetEncoding.offset,
            QNN_TENSOR_GET_QUANT_PARAMS(logitsOutputTensor).scaleOffsetEncoding.scale,
            vocab_size);
    } else {
        LOGE("Unsupported data type");
        return RWKV_ERROR_IO;
    }

    return RWKV_SUCCESS;
}

int qnn_backend::eval(std::vector<int> ids, std::vector<float> &logits) {
    if (ids.empty()) return RWKV_ERROR_EVAL;
    if (prefillSequenceLength == 0) {
        for (auto id : ids) {
            if (RWKV_SUCCESS != eval(id, logits)) {
                return RWKV_ERROR_MODEL;
            }
        }
    } else {
        int *token_input = (int*)qnnIOTensorUtils.getBuffer(&inputTensorsPrefill[0][0]);
        int idx = 0;
// temporarily disable prefilling
#if 0
        for (; (idx + prefillSequenceLength) < ids.size(); idx += prefillSequenceLength) {
            for (int i = 0; i < prefillSequenceLength; i++) {
                *token_input = ids[idx + i];
            }
            LOGI("Prefilling using seq mode from %d to %d", idx, idx + prefillSequenceLength);

            for (int graph_id = 0; graph_id < qnnPrefillGraphsCount; graph_id++) {
                auto graphInfo     = (*qnnPrefillGraphsInfo)[graph_id];
                auto executeStatus =
                    qnnFunctionPointers.qnnInterface.graphExecute(graphInfo.graph,
                                                                    inputTensorsPrefill[graph_id],
                                                                    graphInfo.numInputTensors,
                                                                    outputTensorsPrefill[graph_id],
                                                                    graphInfo.numOutputTensors,
                                                                    nullptr,
                                                                    nullptr);

                if (QNN_GRAPH_NO_ERROR != executeStatus) {
                    return RWKV_ERROR_IO;
                }
            }
        }
#endif
        for (; idx < ids.size(); idx++) {
            token_input = (int*)qnnIOTensorUtils.getBuffer(&inputTensors[0][0]);
            *token_input = ids[idx];
            LOGI("Prefilling using decode mode at %d", idx);
            for (int graph_id = 0; graph_id < qnnDecodeGraphsCount; graph_id++) {
                auto graphInfo     = (*qnnDecodeGraphsInfo)[graph_id];
                auto executeStatus =
                    qnnFunctionPointers.qnnInterface.graphExecute(graphInfo.graph,
                                                                    inputTensors[graph_id],
                                                                    graphInfo.numInputTensors,
                                                                    outputTensors[graph_id],
                                                                    graphInfo.numOutputTensors,
                                                                    nullptr,
                                                                    nullptr);

                if (QNN_GRAPH_NO_ERROR != executeStatus) {
                    return RWKV_ERROR_IO;
                }
            }
        }   
    }

    // copy logits
    if (logits.empty()) logits = std::vector<float>(vocab_size);

    if (QNN_TENSOR_GET_DATA_TYPE(logitsOutputTensor) == QNN_DATATYPE_FLOAT_32) {
        float *ptr = (float*)QNN_TENSOR_GET_CLIENT_BUF(logitsOutputTensor).data;
        logits = std::vector<float>(ptr, ptr + vocab_size);
    } else if (QNN_TENSOR_GET_DATA_TYPE(logitsOutputTensor) == QNN_DATATYPE_FLOAT_16) {
        half_float::half *ptr = (half_float::half*)QNN_TENSOR_GET_CLIENT_BUF(logitsOutputTensor).data;
        for (int i = 0; i < vocab_size; i++) {
            logits[i] = ptr[i];
        }
    } else if (QNN_TENSOR_GET_DATA_TYPE(logitsOutputTensor) == QNN_DATATYPE_UFIXED_POINT_16) {
        datautil::tfNToFloat<uint16_t>(logits.data(), reinterpret_cast<uint16_t*>(QNN_TENSOR_GET_CLIENT_BUF(logitsOutputTensor).data),
            QNN_TENSOR_GET_QUANT_PARAMS(logitsOutputTensor).scaleOffsetEncoding.offset,
            QNN_TENSOR_GET_QUANT_PARAMS(logitsOutputTensor).scaleOffsetEncoding.scale,
            vocab_size);
    } else {
        LOGE("Unsupported data type");
        return RWKV_ERROR_IO;
    }
    return RWKV_SUCCESS;
}

bool qnn_backend::is_available() {
    // TODO: Detect this
    return true;
}

int qnn_backend::clear_state() {
    if (!isTensorInitialized) return RWKV_SUCCESS;
    for (int graph_id = 0; graph_id < qnnDecodeGraphsCount; graph_id++) {
        auto graphInfo     = (*qnnDecodeGraphsInfo)[graph_id];

        for (size_t i = 0; i < graphInfo.numOutputTensors; i++) {
            size_t element_count = 1;
            for (int j = 0; j < QNN_TENSOR_GET_RANK(outputTensors[graph_id][i]); j++) {
                element_count *= *(QNN_TENSOR_GET_DIMENSIONS(outputTensors[graph_id][i]) + j);
            }
            void *buffer = qnnIOTensorUtils.getBuffer(&outputTensors[graph_id][i]);
            auto tensorName = std::string(QNN_TENSOR_GET_NAME(outputTensors[graph_id][i]));
            int state_num = 0;
            if (tensorName.find("state") != std::string::npos) {
                tensorName = std::stoi(tensorName.substr(0, tensorName.find("_")).substr(5));
            }

            if (state_num % 3 == 1 || QNN_TENSOR_GET_DATA_TYPE(outputTensors[graph_id][i]) == QNN_DATATYPE_FLOAT_16)
                memset(buffer, 0, element_count * sizeof(uint16_t));
            else if (QNN_TENSOR_GET_DATA_TYPE(outputTensors[graph_id][i]) == QNN_DATATYPE_FLOAT_32)
                memset(buffer, 0, element_count * sizeof(float));
            else {
                fill_quantized_tensor(0.0, &outputTensors[graph_id][i]);
            }
        }
    }
    return RWKV_SUCCESS;
}

int qnn_backend::get_state(std::any &state) {
    auto new_state = std::vector<std::vector<uint8_t>>();
    for (int graph_id = 0; graph_id < qnnDecodeGraphsCount; graph_id++) {
        auto graphInfo     = (*qnnDecodeGraphsInfo)[graph_id];

        for (size_t i = 0; i < graphInfo.numOutputTensors; i++) {
            std::string outputName = std::string(QNN_TENSOR_GET_NAME(outputTensors[graph_id][i]));
            
            if (outputName.find("state") != std::string::npos) {
                new_state.push_back(std::vector<uint8_t>((uint8_t*)qnnIOTensorUtils.getBuffer(& outputTensors[graph_id][i]), (uint8_t*)qnnIOTensorUtils.getBuffer(&outputTensors[graph_id][i]) + qnnIOTensorUtils.getBufferSize(&outputTensors[graph_id][i])));
            }
        }
    }
    state = new_state;
    return RWKV_SUCCESS;
}

int qnn_backend::set_state(std::any state) {
    if (!state.has_value()) return RWKV_SUCCESS;
    auto new_state = std::any_cast<std::vector<std::vector<uint8_t>>>(state);
    int idx = 0;
    for (int graph_id = 0; graph_id < qnnDecodeGraphsCount; graph_id++) {
        auto graphInfo     = (*qnnDecodeGraphsInfo)[graph_id];

        for (size_t i = 0; i < graphInfo.numOutputTensors; i++) {
            std::string outputName = std::string(QNN_TENSOR_GET_NAME(outputTensors[graph_id][i]));
            
            if (outputName.find("state") != std::string::npos) {
                memcpy(qnnIOTensorUtils.getBuffer(&outputTensors[graph_id][i]), new_state[idx].data(), new_state[idx].size());
                idx++;
            }
        }
    }
    return RWKV_SUCCESS;
}

int qnn_backend::free_state(std::any state) {
    if (!state.has_value()) return RWKV_SUCCESS;
    auto new_state = std::any_cast<std::vector<std::vector<uint8_t>>>(state);
    for (auto &s : new_state) {
        s.clear();
    }
    new_state.clear();
    return RWKV_SUCCESS;
}

int qnn_backend::release_model() {
    LOGE("qnn_backend::release_model()");
    // free graphs
    for (int i = 0; i < qnnDecodeGraphsCount; i++) {
        auto graphInfo     = (*qnnDecodeGraphsInfo)[i];
        qnnIOTensorUtils.tearDownTensors(inputTensors[i], graphInfo.numInputTensors);
        qnnIOTensorUtils.tearDownTensors(outputTensors[i], graphInfo.numOutputTensors);
        inputTensors[i]  = nullptr;
        outputTensors[i] = nullptr;
    }

    freeGraphsInfo(&qnnDecodeGraphsInfo, qnnDecodeGraphsCount);
    qnnDecodeGraphsInfo = nullptr;

    for (int i = 0; i < qnnPrefillGraphsCount; i++) {
        auto graphInfo     = (*qnnPrefillGraphsInfo)[i];
        qnnIOTensorUtils.tearDownTensors(inputTensorsPrefill[i], graphInfo.numInputTensors);
        qnnIOTensorUtils.tearDownTensors(outputTensorsPrefill[i], graphInfo.numOutputTensors);
        inputTensorsPrefill[i]  = nullptr;
        outputTensorsPrefill[i] = nullptr;
    }

    freeGraphsInfo(&qnnPrefillGraphsInfo, qnnPrefillGraphsCount);
    qnnPrefillGraphsInfo = nullptr;

    // TODO: Why are these qnn release functions crashing?

    // if (QNN_CONTEXT_NO_ERROR !=
    //     qnnFunctionPointers.qnnInterface.contextFree(qnnContextHandles[0], nullptr)) {
    //     LOGE("Could not free context");
    // }

    // qnn_destory_power_config_id();

    // if (nullptr != qnnFunctionPointers.qnnInterface.propertyHasCapability) {
    //     auto qnnDevicePropertyStatus = qnnFunctionPointers.qnnInterface.propertyHasCapability(QNN_PROPERTY_GROUP_DEVICE);
    //     if (QNN_PROPERTY_NOT_SUPPORTED == qnnDevicePropertyStatus) {
    //         LOGW("Device property is not supported");
    //     }

    //     auto qnnStatus = qnnFunctionPointers.qnnInterface.deviceFree(qnnDeviceHandle);
    //     if (QNN_SUCCESS != qnnStatus && QNN_DEVICE_ERROR_UNSUPPORTED_FEATURE != qnnStatus) {
    //         LOGE("Failed to free device");
    //     }
    // }

    if (qnnBackendLibraryHandle)
        pal::dynamicloading::dlClose(qnnBackendLibraryHandle);

    if (qnnModelHandle)
        pal::dynamicloading::dlClose(qnnModelHandle);

    for (int i = 0; i < graphConfigsInfoCount; i++) {
        delete graphConfigsInfo[i];
    }
    delete graphConfigsInfo;

    // if ((nullptr != qnnBackendHandle && nullptr != qnnFunctionPointers.qnnInterface.backendFree) &&
    //     QNN_BACKEND_NO_ERROR != qnnFunctionPointers.qnnInterface.backendFree(qnnBackendHandle)) {
    //     LOGE("Could not terminate QNN backend");
    // }
    // qnnBackendHandle = nullptr;

    // if (nullptr != qnnFunctionPointers.qnnInterface.logFree && nullptr != qnnLogHandle) {
    //     if (QNN_SUCCESS != qnnFunctionPointers.qnnInterface.logFree(qnnLogHandle)) {
    //         LOGW("Unable to terminate logging in the backend.");
    //     }
    // }

    return RWKV_SUCCESS;
}

int qnn_backend::release() {
    return RWKV_SUCCESS;
}

int qnn_backend::qnn_register_op_package(std::string package_path, std::string interface_provider) {
    if (nullptr == qnnFunctionPointers.qnnInterface.backendRegisterOpPackage) {
        LOGE("backendRegisterOpPackageFnHandle is nullptr.");
        return RWKV_ERROR_UNSUPPORTED;
    }
    if (QNN_BACKEND_NO_ERROR != qnnFunctionPointers.qnnInterface.backendRegisterOpPackage(
                qnnBackendHandle,
                package_path.c_str(),
                interface_provider.c_str(),
                nullptr)) {
        LOGE("Could not register Op Package: %s and interface provider: %s",
            package_path.c_str(),
            interface_provider.c_str());
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    LOGI("Registered Op Package: %s and interface provider: %s",
        package_path.c_str(),
        interface_provider.c_str()
    );
    return RWKV_SUCCESS;
}

int qnn_backend::qnn_create_power_config_id() {
    QnnDevice_Infrastructure_t deviceInfra = nullptr;
    Qnn_ErrorHandle_t devErr = qnnFunctionPointers.qnnInterface.deviceGetInfrastructure(&deviceInfra);
    if (devErr != QNN_SUCCESS) {
        LOGE("deviceGetInfrastructure error");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    QnnHtpDevice_Infrastructure_t *htpInfra = static_cast<QnnHtpDevice_Infrastructure_t *>(deviceInfra);
    QnnHtpDevice_PerfInfrastructure_t perfInfra = htpInfra->perfInfra;
    Qnn_ErrorHandle_t perfInfraErr = perfInfra.createPowerConfigId(deviceId, coreId, &powerConfigId);
    if (perfInfraErr != QNN_SUCCESS) {
      LOGE("createPowerConfigId failed");
      return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    return RWKV_SUCCESS;
}

int qnn_backend::qnn_destory_power_config_id() {
    QnnDevice_Infrastructure_t deviceInfra = nullptr;
    Qnn_ErrorHandle_t devErr = qnnFunctionPointers.qnnInterface.deviceGetInfrastructure(&deviceInfra);
    if (devErr != QNN_SUCCESS) {
        LOGE("deviceGetInfrastructure error");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_RELEASE;
    }
    QnnHtpDevice_Infrastructure_t *htpInfra = static_cast<QnnHtpDevice_Infrastructure_t *>(deviceInfra);
    QnnHtpDevice_PerfInfrastructure_t perfInfra = htpInfra->perfInfra;

    Qnn_ErrorHandle_t perfInfraErr = perfInfra.destroyPowerConfigId(powerConfigId);
    if (perfInfraErr != QNN_SUCCESS) {
        LOGE("destroyPowerConfigId failed");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_RELEASE;
    }
    return RWKV_SUCCESS;
}

int qnn_backend::qnn_set_power_config() {
    QnnDevice_Infrastructure_t deviceInfra = nullptr;
    Qnn_ErrorHandle_t devErr = qnnFunctionPointers.qnnInterface.deviceGetInfrastructure(&deviceInfra);
    if (devErr != QNN_SUCCESS) {
        LOGE("device error");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    QnnHtpDevice_Infrastructure_t *htpInfra = static_cast<QnnHtpDevice_Infrastructure_t *>(deviceInfra);
    QnnHtpDevice_PerfInfrastructure_t perfInfra = htpInfra->perfInfra;

    QnnHtpPerfInfrastructure_PowerConfig_t powerConfig;
    memset(&powerConfig, 0, sizeof(powerConfig));
    powerConfig.option                     = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_DCVS_V3;
    powerConfig.dcvsV3Config.dcvsEnable    = 0; //True to enable Dcvs, False to disbale
    powerConfig.dcvsV3Config.setDcvsEnable = 1;
    powerConfig.dcvsV3Config.contextId     = powerConfigId;  //use the power config id created

    // refer QnnHtpPerfInfrastructure.h
    powerConfig.dcvsV3Config.powerMode       = QNN_HTP_PERF_INFRASTRUCTURE_POWERMODE_PERFORMANCE_MODE;
    powerConfig.dcvsV3Config.setSleepLatency = 1; //True to consider Latency parameter otherwise False
    powerConfig.dcvsV3Config.setBusParams    = 1; //True to consider Bus parameter otherwise False
    powerConfig.dcvsV3Config.setCoreParams   = 1; //True to consider Core parameter otherwise False
    powerConfig.dcvsV3Config.sleepDisable    = 1; //True to disable sleep, False to re-enable sleep
    powerConfig.dcvsV3Config.setSleepDisable = 1; //True to consider sleep disable/enable parameter otherwise False

    //Set Sleep latency parameter
    powerConfig.dcvsV3Config.sleepLatency    =  40; // set dsp sleep latency ranges 10-65535 micro sec, refer hexagon sdk

    //set Bus Clock Parameters (refer QnnHtpPerfInfrastructure.h)
    powerConfig.dcvsV3Config.busVoltageCornerMin     = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;
    powerConfig.dcvsV3Config.busVoltageCornerTarget  = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;
    powerConfig.dcvsV3Config.busVoltageCornerMax     = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;

    //set Core Clock Parameters (refer QnnHtpPerfInfrastructure.h)
    powerConfig.dcvsV3Config.coreVoltageCornerMin    = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;
    powerConfig.dcvsV3Config.coreVoltageCornerTarget = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;
    powerConfig.dcvsV3Config.coreVoltageCornerMax    = DCVS_VOLTAGE_VCORNER_MAX_VOLTAGE_CORNER;

    QnnHtpPerfInfrastructure_PowerConfig_t powerConfigHMX;
    memset(&powerConfigHMX, 0, sizeof(powerConfigHMX));
    powerConfigHMX.option                     = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_HMX_V2;
    powerConfigHMX.hmxV2Config.hmxPickDefault = 0;
    powerConfigHMX.hmxV2Config.hmxPerfMode    = QNN_HTP_PERF_INFRASTRUCTURE_CLK_PERF_HIGH;

    powerConfigHMX.hmxV2Config.hmxVoltageCornerMin    = DCVS_EXP_VCORNER_TUR;
    powerConfigHMX.hmxV2Config.hmxVoltageCornerTarget = DCVS_EXP_VCORNER_TUR;
    powerConfigHMX.hmxV2Config.hmxVoltageCornerMax    = DCVS_EXP_VCORNER_TUR;

    // Set power config with different performance parameters
    const QnnHtpPerfInfrastructure_PowerConfig_t *powerConfigs[] = {&powerConfig, &powerConfigHMX, NULL};

    Qnn_ErrorHandle_t perfInfraErr = perfInfra.setPowerConfig(powerConfigId, powerConfigs);
    if (perfInfraErr != QNN_SUCCESS) {
        LOGE("setPowerConfig failed");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    return RWKV_SUCCESS;
}

int qnn_backend::qnn_set_rpc_latency_and_polling() {
    QnnDevice_Infrastructure_t deviceInfra = nullptr;
    Qnn_ErrorHandle_t devErr = qnnFunctionPointers.qnnInterface.deviceGetInfrastructure(&deviceInfra);
    if (devErr != QNN_SUCCESS) {
      LOGE("device error");
      return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    QnnHtpDevice_Infrastructure_t *htpInfra = static_cast<QnnHtpDevice_Infrastructure_t *>(deviceInfra);
    QnnHtpDevice_PerfInfrastructure_t perfInfra = htpInfra->perfInfra;

    // set RPC Control Latency
    QnnHtpPerfInfrastructure_PowerConfig_t rpcControlLatency;            // refer QnnHtpPerfInfrastructure.h
    memset(&rpcControlLatency, 0, sizeof(rpcControlLatency));
    rpcControlLatency.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_RPC_CONTROL_LATENCY;
    rpcControlLatency.rpcControlLatencyConfig = 100;         // use rpc control latency recommended 100 us, refer hexagon sdk
    const QnnHtpPerfInfrastructure_PowerConfig_t *powerConfigs1[] = {&rpcControlLatency, NULL};

    Qnn_ErrorHandle_t perfInfraErr = perfInfra.setPowerConfig(powerConfigId, powerConfigs1);  // set RPC latency config on power config id created
    if (perfInfraErr != QNN_SUCCESS) {
        LOGE("setPowerConfig failed");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }

    // set RPC Polling
    QnnHtpPerfInfrastructure_PowerConfig_t rpcPollingTime;   // refer QnnHtpPerfInfrastructure.h
    memset(&rpcPollingTime, 0, sizeof(rpcPollingTime));
    rpcPollingTime.option = QNN_HTP_PERF_INFRASTRUCTURE_POWER_CONFIGOPTION_RPC_POLLING_TIME;
    rpcPollingTime.rpcPollingTimeConfig = 9999;     // use rpc polling time recommended 0-10000 us
    const QnnHtpPerfInfrastructure_PowerConfig_t* powerConfigs2[] = {&rpcPollingTime, NULL};

    perfInfraErr = perfInfra.setPowerConfig(powerConfigId, powerConfigs2); // set RPC polling config on power config id created
    if (perfInfraErr != QNN_SUCCESS) {
        LOGE("setPowerConfig failed");
        return RWKV_ERROR_BACKEND | RWKV_ERROR_INIT;
    }
    return RWKV_SUCCESS;
}

#else

int qnn_backend::init(void * extra) {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::load_model(std::string model_path) {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::eval(int id, std::vector<float> &logits) {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::eval(std::vector<int> ids, std::vector<float> &logits) {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::clear_state() {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::get_state(std::any &state) {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::set_state(std::any state) {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::free_state(std::any state) {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

bool qnn_backend::is_available() {
    return false;
}

int qnn_backend::release_model() {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

int qnn_backend::release() {
    return RWKV_ERROR_BACKEND | RWKV_ERROR_UNSUPPORTED;
}

#endif

} // namespace rwkvmobile